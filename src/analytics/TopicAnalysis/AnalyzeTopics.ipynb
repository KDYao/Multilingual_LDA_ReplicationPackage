{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import src.resources.utils.utils as ut"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "global out_dir_root\n",
    "global data_root_dir\n",
    "global top_nterms_from_topic\n",
    "global section_name"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "# Specs\n",
    "# Define where to load metrics\n",
    "data_root_dir = ut.getPath('ROOT')\n",
    "res_root_dir = os.path.join(ut.get_project_root(), 'out')\n",
    "\n",
    "# Now only support LDA1/LDA2\n",
    "evaluate_metrics = ['PERPLEXITY', 'COHERENCE', 'INTERCORPUSRATIO', 'AGGREGATED']\n",
    "evaluate_ldas = ['LDA1', 'LDA2', 'LDA3']\n",
    "top_nterms_from_topic = 20\n",
    "section_name = 'am-i-infected-what-do-i-do'\n",
    "\n",
    "f_data_dict_name = ut.getPath('CORPUS_NAME')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "def loadDataDict():\n",
    "    f_data_dict = os.path.join(*[data_root_dir, 'post-processed-data', f_data_dict_name])\n",
    "    with open(f_data_dict, 'rb') as pkl:\n",
    "        data_dict = pickle.load(pkl)\n",
    "    return data_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "def loadCorpus(lda_type, f_model):\n",
    "    # TODO: Add support for LDA3\n",
    "    if not os.path.isfile(f_model):\n",
    "        # log.error('Fail to find document file %s', f_model)\n",
    "        raise FileNotFoundError('Fail to find document file %s' % f_model)\n",
    "\n",
    "    if lda_type == 'LDA1':\n",
    "        f = os.path.join(*[data_root_dir, 'documents', lda, '{}_documents.pkl'.format(lda)])\n",
    "        with open(f, 'rb') as handle:\n",
    "            return pickle.load(handle)\n",
    "    else:\n",
    "        f_temp = os.path.join(*[data_root_dir, 'documents', lda, '{}_{}_documents.pkl'])\n",
    "        corpus = []\n",
    "        documents = []\n",
    "        dictionaries = []\n",
    "        # Load documents for both log and text\n",
    "        for f in map(lambda x: f_temp.format(lda, x), ('log', 'text')):\n",
    "            with open(f, 'rb') as handle:\n",
    "                f_documents, f_corpus, f_dictionary = pickle.load(handle)\n",
    "                corpus.append(f_corpus)\n",
    "                documents.append(f_documents)\n",
    "                dictionaries.append(f_dictionary)\n",
    "        # Update documents by merging log/text documents\n",
    "        documents = [x + y for x, y in zip(documents[0], documents[1])]\n",
    "        text_corpus = corpus.pop(1)\n",
    "        # Assign log_corpus to corpus\n",
    "        corpus = corpus[0]\n",
    "        log_num_terms = len(dictionaries[0])\n",
    "        for idx, text_corpus_row in enumerate(text_corpus):\n",
    "            corpus[idx] += [(tp[0] + log_num_terms, tp[1]) for tp in text_corpus_row]\n",
    "        # Do not need dictionary, it can be captured from model.id2word\n",
    "        return documents, corpus, None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "def loadModel(f_model, corpus):\n",
    "    \"\"\"\n",
    "    Load lda model\n",
    "    Parameters\n",
    "    ----------\n",
    "    f_model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    with open(f_model, 'rb') as pkl:\n",
    "        ldamodel = pickle.load(pkl)\n",
    "    topic_per_doc = {}\n",
    "    tmp_doc_prob = {}\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                # if top_nterms_from_topic:\n",
    "                wp = ldamodel.show_topic(topic_num, topn=top_nterms_from_topic)\n",
    "                # else:\n",
    "                #     wp = ldamodel.show_topic(topic_num)\n",
    "                top_topic_keywords = [word for word, prop in wp]\n",
    "                topic_per_doc[i] = {'Topic': topic_num,\n",
    "                                    'TopTermsRough': top_topic_keywords,\n",
    "                                    'TopTerms': [re.sub(';TOPIC:\\d+', '', x.replace('TEXT:', '').replace('LOG:', ''))\n",
    "                                                 for x in top_topic_keywords],\n",
    "                                    'Theta': prop_topic}\n",
    "                tmp_doc_prob[i] = prop_topic\n",
    "                break\n",
    "            else:\n",
    "                break\n",
    "    # Sort by topic-document matching probability\n",
    "    topic_per_doc = dict(sorted(topic_per_doc.items(), key=lambda tup: tup[1]['Theta'], reverse=True))\n",
    "\n",
    "    return topic_per_doc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "def load_topic_from_txt(postIdMathching, top_num_terms=10):\n",
    "    \"\"\"\n",
    "    Load top terms per topic for all documents\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    tmp_out = os.path.join(*[res_root_dir, metric, 'model', lda])\n",
    "    f_doc_topics = os.path.join(tmp_out, 'doctopics.txt')\n",
    "    f_topic_keys = os.path.join(tmp_out, 'topickeys.txt')\n",
    "    d_html = os.path.join(*[data_root_dir,\n",
    "                            'raw-data',\n",
    "                            'BleepingComputerHtmlFiles',\n",
    "                            section_name])\n",
    "    lst_topic_terms = []\n",
    "\n",
    "    # Read top terms per topic\n",
    "    # Noted some topic has less than 10 terms (default top N words from mallet output is 20)\n",
    "    # mt_topic_keys = np.loadtxt(f_topic_keys, dtype=\"str\", usecols=range(2, 2 + topN))\n",
    "    with open(f_topic_keys, 'r') as fr:\n",
    "        for idx, line in enumerate(fr):\n",
    "            if line.strip() == '': continue\n",
    "            words = line.strip('\\n').split()[2:]\n",
    "            top_terms = words[:min(top_num_terms, len(words))]\n",
    "            #terms = ['{term};TOPIC:{topicID}'.format(topicID=str(idx), term=x) for x in top_terms]\n",
    "\n",
    "            lst_topic_terms.append({'Topic': idx,\n",
    "                                    'TopTermsRough': top_terms,\n",
    "                                    'TopTerms': [re.sub(';TOPIC:\\d+', '', x.replace('TEXT:', '').replace('LOG:', '')) for x in top_terms]\n",
    "                                    })\n",
    "\n",
    "    num_topics = len(lst_topic_terms)\n",
    "    # Load document-topic\n",
    "    mt_doc_topics = np.loadtxt(f_doc_topics, dtype=\"float\", usecols=range(2, 2 + num_topics))\n",
    "    max_theta_indexes = np.argmax(mt_doc_topics, axis=1)\n",
    "\n",
    "    res = []\n",
    "    for doc_id, topic_id in enumerate(max_theta_indexes):\n",
    "        topic_dict = lst_topic_terms[topic_id]\n",
    "        post_id = postIdMathching[doc_id]\n",
    "        # f_htmls = [\n",
    "        #     x for x in os.listdir(d_html) if x.startswith('tid-link-%d' % post_id)\n",
    "        # ]\n",
    "        # TODO: Collect from remote server\n",
    "\n",
    "        theta = mt_doc_topics[doc_id, topic_id]\n",
    "        res.append({\n",
    "            **topic_dict, **{\n",
    "                'PostId': post_id,\n",
    "                'Theta': theta\n",
    "            }\n",
    "        })\n",
    "    res = sorted(res, key=lambda x: x['Theta'], reverse=True)\n",
    "    return pd.DataFrame(res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "def load_topic_from_model(postIdMathching):\n",
    "    \"\"\"\n",
    "    Load top terms per topic by stored model\n",
    "    This function is slower so use another load topic function\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    f_model = os.path.join(*[res_root_dir, metric, 'model', lda, 'mallet_model.pkl'])\n",
    "    # Check file\n",
    "    if not os.path.isfile(f_model):\n",
    "        print('Fail to find file %s, continue' % f_model)\n",
    "        return\n",
    "    # Load model\n",
    "    documents, corpus, _ = loadCorpus(lda_type=lda, f_model=f_model)\n",
    "    topic_per_doc = loadModel(f_model, corpus)\n",
    "\n",
    "    topic_per_doc_list = []\n",
    "    for docId, topic_info in topic_per_doc.items():\n",
    "        topic_info['PostId'] = postIdMathching[docId]\n",
    "        topic_per_doc_list.append(topic_info)\n",
    "\n",
    "    f_output = os.path.join(*[data_root_dir, 'result', 'matching',\n",
    "                                'top' + str(top_nterms_from_topic) if top_nterms_from_topic else 'all',\n",
    "                          'top_terms_per_doc_to_html_%s_%s.csv' % (lda, metric)])\n",
    "    if not os.path.isdir(os.path.dirname(f_output)): os.makedirs(os.path.dirname(f_output))\n",
    "\n",
    "    # df = pd.DataFrame(topic_per_doc).T\n",
    "    # df.to_csv(f_output, index_label='PostId')\n",
    "    # df['PostId'] = df.index\n",
    "    # df.to_csv(f_output, index=False)\n",
    "\n",
    "    df = pd.DataFrame(topic_per_doc_list)\n",
    "    df.to_csv(f_output, index=False)\n",
    "    print('Top terms per doc are written into %s' % f_output)\n",
    "\n",
    "    # out_dir = os.path.join(*[res_root_dir, 'HTMLs', lda])\n",
    "\n",
    "    # if isShowHTML:\n",
    "    #     highlightTopicTokensByDocs(data_dict, topic_per_doc, ntop=top_nterms_from_topic)\n",
    "    #\n",
    "    # # postIdToDocTerms is the third item of data_dict which stores the mapping\n",
    "    # for section in data_dict.keys():\n",
    "    #     postIds = data_dict[section][2].keys()\n",
    "    #     saveToMd(documents, postIds, topic_per_doc, 10, isShowHTML)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_dict = loadDataDict()\n",
    "postIdMathching = [post_id for post_id in data_dict[section_name][0].keys()]\n",
    "isShowHTML = False\n",
    "for metric in evaluate_metrics:\n",
    "    for lda in evaluate_ldas:\n",
    "        print('Processing {}'.format(lda))\n",
    "        out_dir_root = os.path.join(*[res_root_dir, metric, '{}', lda])\n",
    "        if top_nterms_from_topic:\n",
    "            # If not None\n",
    "            if top_nterms_from_topic <= 20:\n",
    "                try:\n",
    "                    df_topic_top_terms = load_topic_from_txt(postIdMathching=postIdMathching, top_num_terms=top_nterms_from_topic)\n",
    "                    f_output = os.path.join(*[data_root_dir, 'result', 'matching',\n",
    "                                       'top_terms_per_doc_to_html_%s_%s.csv' % (lda, metric)])\n",
    "                    if not os.path.isdir(os.path.dirname(f_output)): os.makedirs(os.path.dirname(f_output))\n",
    "                    df_topic_top_terms.to_csv(f_output, index=False)\n",
    "                    print('Writing to %s' % f_output)\n",
    "                except FileNotFoundError as e:\n",
    "                    print('Fail to load from TXT. Now load from model.')\n",
    "                    load_topic_from_model(postIdMathching)\n",
    "            else:\n",
    "                print('Cannot load more than top 20 words from TXT. Now load from model.')\n",
    "                load_topic_from_model(postIdMathching)\n",
    "        else:\n",
    "            print('Number of top terms not specified. Will load all terms per topic from model.')\n",
    "            load_topic_from_model(postIdMathching)\n",
    "print('Finished')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}